{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df23e646",
   "metadata": {},
   "source": [
    "# Tanzania Primary Education Results (NECTA PSLE)\n",
    "\n",
    "### 2a. Data Sourcing - NECTA\n",
    "1. Beautiful Soup webscrape of NECTA data\n",
    "\n",
    "#### Outputs:\n",
    "* nation_necta_raw.csv (17935, 10)\n",
    "\n",
    "#### Functions:\n",
    "* `scrape_psle_page`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5cc2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries: pre-installed in Anaconda\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#User-defined functions.py\n",
    "import functions as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297d15fa",
   "metadata": {},
   "source": [
    "### 1. Beautiful Soup webscrape of NECTA data\n",
    "**ELI5 Summary:**\n",
    "*Get primary school examination results from public web page for each school*\n",
    "\n",
    "**Steps:**\n",
    "1. Get region links from national page of [PSLE results](https://onlinesys.necta.go.tz/results/2022/psle/psle.htm)\n",
    "2. Get council (districts or municipalities) links from regional page\n",
    "3. Get individual school links from council page\n",
    "4. Call function <code>scrape_psle_page</code> to get **dict of school data**\n",
    "5. Add region and council name to dict from looping code\n",
    "6. Append THIS school's data to **list of dicts**\n",
    "7. After three-level loop: create **Pandas DataFrame** from list of dicts and save to CSV\n",
    "\n",
    "**DATA observations:**\n",
    "* Example: [Jitegmee Primary School](https://onlinesys.necta.go.tz/results/2022/psle/results/shl_ps1104063.htm)    \n",
    "* School level data is in text below headers and first table (genders/total x grades)\n",
    "    * Grade (DARAJA) is based on school average out of 300 points (6 subjects x 50 points)\n",
    "    * Per-student, per-subject data not used (for now)\n",
    "* @nation: **17,935 school pages** scraped\n",
    "\n",
    "**Corner cases:** (DEBUG CODE)\n",
    "* 'SEMINARY' in school name (3) => `scrape_psle_page` regex\n",
    "* typo ';' before NECTA exam id (1) => `scrape_psle_page` regex\n",
    "\n",
    "**Learnings:** (üßëüèª‚Äçüíªüìöüòé‚ö†Ô∏è)\n",
    "- üßëüèª‚Äçüíª Collect all webscarped data into a **list of dicts** first then write to a DataFrame once afterwards\n",
    "    - ‚ö†Ô∏è Avoid iterative Pandas concat\n",
    "- üìö Scaling up to full (national) dataset (n=17k) forced me to think differently. Finding specific corner casess needs to be scalable/automated (code) vs. specific/manual (eye-ball).\n",
    "- üòé HTML code is very structured and BeautifulSoup can easily \"find\" tags on a page (didn't need to \"navigate\" through HTML)\n",
    "- üòé Iterate down through **multi-level page hierarchy**: get page \"soup\", get links > get next-level page \"soup\"\n",
    "- üòé **Regular expressions** are a joy to use with this very helpful website to test: [regex101](https://regex101.com/)\n",
    "    - ‚ö†Ô∏è Needed full dataset to catch all string matching cases, e.g., 'SEMINARY' instead of 'PRIMARY SCHOOL'\n",
    "- ‚ö†Ô∏è Numerous requests (HTTP GET) to same server caused **\"Max retries exceeded with url\"**\n",
    "    - üßëüèª‚Äçüíª File I/O for DEBUG: <code>with open</code> construct using over('w')rite and ('a')ppend modes\n",
    "    - üßëüèª‚Äçüíª Exception handling for DEBUG: <code>try: ... except: ...</code> block\n",
    "    - üßëüèª‚Äçüíª **SOLUTION: timeout and retries** in <code>scrape_psle_page</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7effed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Webscrape down through page levels: region > council > school\n",
    "\n",
    "#Base information\n",
    "exam = 'psle'\n",
    "year = '2022'\n",
    "base_URL = f\"https://onlinesys.necta.go.tz/results/{year}/{exam}/\"\n",
    "top_URL = f\"{exam}.htm\"\n",
    "URL = base_URL + top_URL\n",
    "\n",
    "#Needed for full dataset webscraping\n",
    "timeout = 10 #requests.get timeout\n",
    "retries = 10 #Retry class\n",
    "\n",
    "#DEBUG CODE\n",
    "with open('textout/http_responses.txt', 'w') as f:\n",
    "    f.write(f\"HTTP Responses\\n\")\n",
    "with open('textout/regex_mismatches.txt', 'w') as f:\n",
    "    f.write(f\"Regex Mismatches\\n\")\n",
    "\n",
    "#National level scrape\n",
    "response = requests.get(URL)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "#Initialize list of schools data (dictionaries)\n",
    "y = list()\n",
    "\n",
    "regions = soup.find_all('a') #just find all hyperlink tags!\n",
    "for r in regions:\n",
    "    region_name = r.get_text(strip=True).capitalize() #match TAMISEMI\n",
    "    region_URL = r['href']\n",
    "        \n",
    "    #Regional level scrape\n",
    "    response = requests.get(base_URL + region_URL)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    councils = soup.find_all('a')\n",
    "\n",
    "    for c in councils:    \n",
    "        council_name = c.get_text(strip=True).capitalize() #match TAMISEMI\n",
    "        m = re.search('(.*)\\s+(\\w{2})', council_name) #e.g., handle 'MOROGORO MC' > 'Morogoro MC'\n",
    "        if m != None: council_name = f\"{m[1]} {m[2].upper()}\" \n",
    "\n",
    "        council_URL = \"results/\" + c['href']\n",
    "\n",
    "        #Council (District/Municipality) level scrape\n",
    "        response = requests.get(base_URL + council_URL)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        schools = soup.find_all('a')\n",
    "\n",
    "        for s in schools:\n",
    "            school_name = s.get_text(strip=True)\n",
    "            school_URL = s['href']\n",
    "\n",
    "            #School level scrape in function > returns dict of school data\n",
    "            x = fn.scrape_psle_page(school_URL, timeout, retries)\n",
    "            #Add supplemental info from higher pages\n",
    "            x['region_name'] = region_name\n",
    "            x['council_name'] = council_name\n",
    "            #Add to list of schools\n",
    "            y.append(x)\n",
    "\n",
    "        #Leoson idea: add delay vs. server traffic limiting\n",
    "        #sleep(60*1) #unit in seconds\n",
    "\n",
    "#One time creation of DataFrame-NECTA (df_n) from list of dicts\n",
    "df_n = pd.DataFrame.from_records(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4522da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEBUG CODE\n",
    "#Patching from regex_mismatches.txt => scrape_psle_page: regex update to:\n",
    "# - include 'SEMINARY' after school name (3)\n",
    "# - allow typos before NECTA exam id (1)\n",
    "\n",
    "#check MISSING schools\n",
    "df_n[df_n['school_name'].isna()]\n",
    "\n",
    "#df_n[df_n['school_name'].isna()]\n",
    "i = df_n[df_n['school_name'].isna()].index #2928, 3638, 11890, 15076\n",
    "regex_mismatches_list = [(i[0], 'shl_ps0502150.htm'),\\\n",
    "                         (i[1], 'shl_ps0505121.htm'),\\\n",
    "                         (i[2], 'shl_ps1701079.htm'),\\\n",
    "                         (i[3], 'shl_ps2101141.htm')]\n",
    "for schools in regex_mismatches_list:\n",
    "    x = scrape_psle_page(schools[1], 10, 10)\n",
    "    df_n.at[schools[0], 'school_name'] = x['school_name']\n",
    "    df_n.at[schools[0], 'school_id'] = x['school_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ab3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPOT-CHECK CODE - handy, keep around!\n",
    "df_n.shape\n",
    "#df_n.head()\n",
    "\n",
    "#Save to CSV #@nation: 102ms\n",
    "#df_n.to_csv('dataout/2a/nation_necta_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29054ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
